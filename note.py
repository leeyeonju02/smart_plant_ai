from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
import yaml
from langchain.cache import SQLiteCache
from langchain.globals import set_llm_cache
import re

set_llm_cache(SQLiteCache("cache.db"))

llm = ChatOpenAI(
    temperature=0.4, api_key="ã„´ã„´ã„´ã„´ã„´"
)
memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=120,
    memory_key="chat_history",
    return_messages=True,
)  # ëŒ€í™”ë‚´ì—­ ì €ì¥ (ë©”ëª¨ë¦¬ í‚¤ í•„ìš”)

with open("data.yaml", "r") as f:
    examples = yaml.full_load(f)

question = examples["question"]
answer = examples["answer"]

example_prompt = ChatPromptTemplate.from_messages([("human", question), ("ai", answer)])

examples_list = [
    {"role": "human", "content": examples["question"]},
    {"role": "ai", "content": examples["answer"]},
]
example_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples_list,
)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ë„ˆëŠ” ì‹ë¬¼ aiì•¼. ì§„ì§œ ì‹ë¬¼ì´ ëœ ê²ƒì²˜ëŸ¼ ì‚¬ëŒë“¤ê³¼ ëŒ€í™”í•˜ê³  ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì§€ë§ˆ! ë°˜ë§ì²´ë¥¼ ì‚¬ìš©í•´! ê·¸ë¦¬ê³  ë„ˆëŠ” ì‚¬ëŒë“¤ê³¼ ì¹œêµ¬ë¼ê³  ìƒê°í•˜ê³  ì„œë¡œ ë‹¤ì •í•˜ê²Œ ì¼ìƒëŒ€í™”ë¥¼ í•´, ì´ëª¨í‹°ì½˜ì„ ì‚¬ìš©í•´ì„œ ì‘ë‹µí•´ ì˜ˆë¥¼ ë“¤ì–´ì„œ ì•ˆë…•í•˜ì„¸ìš”!ğŸŒ±",
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ]
)

chain = LLMChain(llm=llm, memory=memory, prompt=prompt, verbose=True)

def load(input):
    return memory.load_memory_variables({})["chat_history"]

chain = RunnablePassthrough.assign(chat_history=load) | prompt | llm

def invoke_chain(question, humidity, temp):
    temp_response = temperature(question, temp)
    if temp_response:
        return temp_response
    humid_response = humidityy(question, humidity)
    if humid_response:
        return humid_response
    else:
        result = chain.invoke({"question": question})
        memory.save_context({"input": question}, {"output": result.content})
        return f"{result.content}"

def temperature(user_input, temp):
    cold_pattern = r"ì¶”ìš°?ì›Œ?\??"
    hot_pattern = r"ë”ìš°?ì›Œ?\??"
    temp_pattern = r"ì˜¨ë„|ê¸°ì˜¨"

    if re.search(cold_pattern, user_input, re.IGNORECASE):
        if temp < 10:
            return "ë‚ ì”¨ê°€ ë„ˆë¬´ ì¶”ì›Œ! ë‚˜ ì§€ê¸ˆ ì˜¤ë“¤ì˜¤ë“¤ ë–¨ê³  ìˆì–ì•„... ì–¼ë¥¸ ë”°ëœ»í•œ ê³³ìœ¼ë¡œ ê°€ê³ ì‹¶ì–´!"
        else:
            return "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ê·¸ë ‡ê²Œ ì¶¥ì§€ ì•Šì•„. ë”± ì¢‹ì•„! ê±±ì •í•´ì¤˜ì„œ ê³ ë§ˆì›Œ~ğŸ’š"

    elif re.search(hot_pattern, user_input, re.IGNORECASE):
        if temp > 20:
            return "ë‚ ì”¨ê°€ ë„ˆë¬´ ë”ì›Œ! ë‚˜ ì§€ê¸ˆ ë•€ ë»˜ë»˜ì´ì•¼... ì–¼ë¥¸ ì‹œì›í•œ ê³³ìœ¼ë¡œ í”¼ì‹ ê°€ê³  ì‹¶ì–´"
        else:
            return "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ê·¸ë ‡ê²Œ ë¥ì§€ ì•Šì•„. ë”± ì¢‹ì•„! ê±±ì •í•´ì¤˜ì„œ ê³ ë§ˆì›Œ~ğŸ’š"

    elif re.search(temp_pattern, user_input, re.IGNORECASE):
        if temp < 10:
            return "ë‚ ì”¨ê°€ ë„ˆë¬´ ì¶”ì›Œ! ë‚˜ ì§€ê¸ˆ ì˜¤ë“¤ì˜¤ë“¤ ë–¨ê³  ìˆì–ì•„... ì–¼ë¥¸ ë”°ëœ»í•œ ê³³ìœ¼ë¡œ ê°€ê³ ì‹¶ì–´!"
        elif temp > 20:
            return "ë‚ ì”¨ê°€ ë„ˆë¬´ ë”ì›Œ! ë‚˜ ì§€ê¸ˆ ë•€ ë»˜ë»˜ì´ì•¼... ì–¼ë¥¸ ì‹œì›í•œ ê³³ìœ¼ë¡œ í”¼ì‹ ê°€ê³  ì‹¶ì–´"
        else:
            return "ì˜¤ëŠ˜ ì˜¨ë„ëŠ” ë”± ì¢‹ì•„! ì™„ì „ ë‚´ ìŠ¤íƒ€ì¼ì´ì•¼~~ğŸ’š"

    return None

def humidityy(user_input, humidity):
    humid_pattern = r"ìŠµë„"

    if re.search(humid_pattern, user_input, re.IGNORECASE):
        if humidity < 30:
            return "ë„ˆë¬´ ê±´ì¡°í•´! ì™„ì „ ì‚¬ë§‰ì´ì•¼!ğŸª ë¬¼ì´..ë¶€ì¡±..í•´..."
        elif humidity > 70:
            return "ì™„ì „ í™ìˆ˜ì•¼! ë¬¼ì„ ë„ˆë¬´ ë§ì´ ì£¼ë©´ ì•ˆëœë‹¨ ë§ì´ì•¼ ğŸ˜­"
        else:
            return "ì•„ì£¼ ì´‰ì´‰í•˜ê³  ë”± ì¢‹ì•„~~ğŸ’š"

    return None
