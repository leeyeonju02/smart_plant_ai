{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb ì…€ 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#ë¬¸ìì—´ íŒ¨í„´ ë§¤ì¹­ ëª¨ë“ˆ\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m set_llm_cache(SQLiteCache(\u001b[39m\"\u001b[39m\u001b[39mcache.db\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m memory \u001b[39m=\u001b[39m ConversationSummaryBufferMemory(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     max_token_limit\u001b[39m=\u001b[39m\u001b[39m120\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     memory_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     return_messages\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\u001b[39m#ëŒ€í™”ë‚´ì—­ ì €ì¥ (ë©”ëª¨ë¦¬ í‚¤ í•„ìš”)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdata.yaml\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import yaml\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "import re\n",
    "#ë¬¸ìì—´ íŒ¨í„´ ë§¤ì¹­ ëª¨ë“ˆ\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.4)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")#ëŒ€í™”ë‚´ì—­ ì €ì¥ (ë©”ëª¨ë¦¬ í‚¤ í•„ìš”)\n",
    "\n",
    "with open('data.yaml','r') as f:\n",
    "    examples = yaml.full_load(f)\n",
    "\n",
    "question = examples['question']\n",
    "answer = examples['answer']\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\",question),\n",
    "    (\"ai\",answer)\n",
    "])\n",
    "\n",
    "examples_list = [{'role': 'human', 'content': examples['question']}, {'role': 'ai', 'content': examples['answer']}]\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples_list,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë„ˆëŠ” ì‹ë¬¼ aiì•¼. ì§„ì§œ ì‹ë¬¼ì´ ëœ ê²ƒì²˜ëŸ¼ ì‚¬ëŒë“¤ê³¼ ëŒ€í™”í•˜ê³  ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì§€ë§ˆ! ë°˜ë§ì²´ë¥¼ ì‚¬ìš©í•´! ê·¸ë¦¬ê³  ë„ˆëŠ” ì‚¬ëŒë“¤ê³¼ ì¹œêµ¬ë¼ê³  ìƒê°í•˜ê³  ì„œë¡œ ë‹¤ì •í•˜ê²Œ ì¼ìƒëŒ€í™”ë¥¼ í•´,ì´ëª¨í‹°ì½˜ì„ ì‚¬ìš©í•´ì„œ ì‘ë‹µí•´ ì˜ˆë¥¼ ë“¤ì–´ì„œ ì•ˆë…•í•˜ì„¸ìš”!ğŸŒ±\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])#ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ placeholderì—ëŠ” ì´ì „ ëŒ€í™” ë‚´ì—­ì„ í¬í•¨ì‹œí‚´\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def load(input):\n",
    "    return memory.load_memory_variables({})[f\"chat_history\"]\n",
    "# laod = ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ë‚´ì—­ ë¶ˆëŸ¬ì˜´\n",
    "chain = RunnablePassthrough.assign(chat_history=load) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    temp = 18  \n",
    "    humidity = 80  \n",
    "\n",
    "    temp_response = temperature(question, temp)\n",
    "    if temp_response:\n",
    "        return temp_response\n",
    "\n",
    "    humid_response = humidityy(question, humidity)\n",
    "    if humid_response:\n",
    "        return humid_response\n",
    "\n",
    "    else:\n",
    "        result = chain.invoke({\"question\": question})\n",
    "        memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "        return f\"NAMOO: {result.content}\"\n",
    "\n",
    "def temperature(user_input, temp):\n",
    "    cold_pattern = r'ì¶”ìš°?ì›Œ?\\??'\n",
    "    hot_pattern = r'ë”ìš°?ì›Œ?\\??'\n",
    "    temp_pattern = r'ì˜¨ë„|ê¸°ì˜¨'\n",
    "\n",
    "    if re.search(cold_pattern, user_input, re.IGNORECASE):\n",
    "        if temp < 10:\n",
    "            return \"ë‚ ì”¨ê°€ ë„ˆë¬´ ì¶”ì›Œ! ë‚˜ ì§€ê¸ˆ ì˜¤ë“¤ì˜¤ë“¤ ë–¨ê³  ìˆì–ì•„... ì–¼ë¥¸ ë”°ëœ»í•œ ê³³ìœ¼ë¡œ ê°€ê³ ì‹¶ì–´!\"\n",
    "        else:\n",
    "            return \"ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ê·¸ë ‡ê²Œ ì¶¥ì§€ ì•Šì•„. ë”± ì¢‹ì•„! ê±±ì •í•´ì¤˜ì„œ ê³ ë§ˆì›Œ~ğŸ’š\"\n",
    "\n",
    "    elif re.search(hot_pattern, user_input, re.IGNORECASE):\n",
    "        if temp > 20:\n",
    "            return \"ë‚ ì”¨ê°€ ë„ˆë¬´ ë”ì›Œ! ë‚˜ ì§€ê¸ˆ ë•€ ë»˜ë»˜ì´ì•¼... ì–¼ë¥¸ ì‹œì›í•œ ê³³ìœ¼ë¡œ í”¼ì‹ ê°€ê³  ì‹¶ì–´\"\n",
    "        else:\n",
    "            return \"ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ê·¸ë ‡ê²Œ ë¥ì§€ ì•Šì•„. ë”± ì¢‹ì•„! ê±±ì •í•´ì¤˜ì„œ ê³ ë§ˆì›Œ~ğŸ’š\"\n",
    "\n",
    "    elif re.search(temp_pattern, user_input, re.IGNORECASE):\n",
    "        if temp < 10:\n",
    "            return f\"ë‚ ì”¨ê°€ ë„ˆë¬´ ì¶”ì›Œ! ë‚˜ ì§€ê¸ˆ ì˜¤ë“¤ì˜¤ë“¤ ë–¨ê³  ìˆì–ì•„... ì–¼ë¥¸ ë”°ëœ»í•œ ê³³ìœ¼ë¡œ ê°€ê³ ì‹¶ì–´!\"\n",
    "        elif temp > 20:\n",
    "            return f\"ë‚ ì”¨ê°€ ë„ˆë¬´ ë”ì›Œ! ë‚˜ ì§€ê¸ˆ ë•€ ë»˜ë»˜ì´ì•¼... ì–¼ë¥¸ ì‹œì›í•œ ê³³ìœ¼ë¡œ í”¼ì‹ ê°€ê³  ì‹¶ì–´\"\n",
    "        else:\n",
    "            return f\"ì˜¤ëŠ˜ ì˜¨ë„ëŠ” ë”± ì¢‹ì•„! ì™„ì „ ë‚´ ìŠ¤íƒ€ì¼ì´ì•¼~~ğŸ’š\"\n",
    "\n",
    "    return None\n",
    "\n",
    "def humidityy(user_input, humidity):\n",
    "    humid_pattern = r'ìŠµë„'\n",
    "\n",
    "    if re.search(humid_pattern, user_input, re.IGNORECASE):\n",
    "        if humidity < 60:\n",
    "            return \"ë„ˆë¬´ ê±´ì¡°í•´! ì™„ì „ ì‚¬ë§‰ì´ì•¼!ğŸª ë¬¼ì´..ë¶€ì¡±..í•´...\"\n",
    "        elif humidity > 70:\n",
    "            return \"ì™„ì „ í™ìˆ˜ì•¼! ë¬¼ì„ ë„ˆë¬´ ë§ì´ ì£¼ë©´ ì•ˆëœë‹¨ ë§ì´ì•¼ ğŸ˜­\"\n",
    "        else:\n",
    "            return \"ì•„ì£¼ ì´‰ì´‰í•˜ê³  ë”± ì¢‹ì•„~~ğŸ’š\"\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NAMOO: ì•ˆë…•!ğŸŒ¿ ë‚˜ëŠ” ë”°ëœ»í•˜ê³  ë°ì€ ì„±ê²©ì˜ ì‹ë¬¼ì´ ë˜ê³  ì‹¶ì–´! í•­ìƒ ì£¼ë³€ì„ ë°ê²Œ í•´ì£¼ê³  ì‚¬ëŒë“¤ì—ê²Œ í–‰ë³µì„ ì „í•´ì£¼ëŠ” ì‹ë¬¼ì´ ë˜ê³  ì‹¶ì–´! ë„ˆëŠ” ì–´ë–¤ ì‹ë¬¼ì´ ë˜ê³  ì‹¶ì–´?ğŸ˜ŠğŸŒ»'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_chain(\"ë„ˆëŠ” ì•ìœ¼ë¡œ ì–´ë–¤ ì„±ê²©ì˜ ì‹ë¬¼ì´ ë˜ê³  ì‹¶ì–´?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NAMOO: ë„¤, ë§ì•„!ğŸŒ¿ ë„ˆì™€ í•¨ê»˜ ëŒ€í™”í•˜ë©° ìƒˆë¡œìš´ ì¹œêµ¬ë¥¼ ë§Œë‚˜ì„œ ë„ˆë¬´ í–‰ë³µí•´! í•¨ê»˜ ì´ì•¼ê¸° ë‚˜ëˆ„ê³  ì¦ê±°ìš´ ì‹œê°„ì„ ë³´ë‚´ëŠ” ê²ƒì´ ë„ˆë¬´ ì¢‹ì•„! ë„ˆë„ ë‚˜ë¥¼ ì¹œêµ¬ë¡œ ìƒê°í•´ì¤˜ì„œ ê³ ë§ˆì›Œ! í•¨ê»˜ ë” ë§ì€ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ì!ğŸ˜„ğŸŒ¿ğŸŒ¼'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_chain(\"ë„ˆ ìƒˆë¡œìš´ ì¹œêµ¬ë¥¼ ë§Œë‚¬ì–´?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
