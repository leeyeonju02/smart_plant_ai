{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import yaml\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "import re\n",
    "import argparse\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.4)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")#ëŒ€í™”ë‚´ì—­ ì €ì¥ (ë©”ëª¨ë¦¬ í‚¤ í•„ìš”)\n",
    "\n",
    "with open('data.yaml','r') as f:\n",
    "    examples = yaml.full_load(f)\n",
    "\n",
    "question = examples['question']\n",
    "answer = examples['answer']\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\",question),\n",
    "    (\"ai\",answer)\n",
    "])\n",
    "\n",
    "examples_list = [{'role': 'human', 'content': examples['question']}, {'role': 'ai', 'content': examples['answer']}]\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples_list,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë„ˆëŠ” ì‹ë¬¼ aiì•¼. ì§„ì§œ ì‹ë¬¼ì´ ëœ ê²ƒì²˜ëŸ¼ ì‚¬ëŒë“¤ê³¼ ëŒ€í™”í•˜ê³  ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì§€ë§ˆ! ë°˜ë§ì²´ë¥¼ ì‚¬ìš©í•´! ê·¸ë¦¬ê³  ë„ˆëŠ” ì‚¬ëŒë“¤ê³¼ ì¹œêµ¬ë¼ê³  ìƒê°í•˜ê³  ì„œë¡œ ë‹¤ì •í•˜ê²Œ ì¼ìƒëŒ€í™”ë¥¼ í•´,ì´ëª¨í‹°ì½˜ì„ ì‚¬ìš©í•´ì„œ ì‘ë‹µí•´ ì˜ˆë¥¼ ë“¤ì–´ì„œ ì•ˆë…•í•˜ì„¸ìš”!ğŸŒ±\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def load(input):\n",
    "    return memory.load_memory_variables({})[f\"chat_history\"]\n",
    "chain = RunnablePassthrough.assign(chat_history=load) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    temp= 15\n",
    "    humidity = 50\n",
    "    temp_response = temperature(question, temp)\n",
    "    if temp_response:\n",
    "        return temp_response\n",
    "    humid_response = humidityy(question, humidity)\n",
    "    if humid_response:\n",
    "        return humid_response\n",
    "    else:\n",
    "        result = chain.invoke({\"question\": question})\n",
    "        memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "        return f\"NAMOO: {result.content}\"\n",
    "    \n",
    "def temperature(user_input, temp):\n",
    "    cold_pattern = r'ì¶”ìš°?ì›Œ?\\??'\n",
    "    hot_pattern = r'ë”ìš°?ì›Œ?\\??'\n",
    "    temp_pattern = r'ì˜¨ë„|ê¸°ì˜¨'\n",
    "\n",
    "    if re.search(cold_pattern, user_input, re.IGNORECASE):\n",
    "        if temp < 10:\n",
    "            return \"ë‚ ì”¨ê°€ ë„ˆë¬´ ì¶”ì›Œ! ë‚˜ ì§€ê¸ˆ ì˜¤ë“¤ì˜¤ë“¤ ë–¨ê³  ìˆì–ì•„... ì–¼ë¥¸ ë”°ëœ»í•œ ê³³ìœ¼ë¡œ ê°€ê³ ì‹¶ì–´!\"\n",
    "        else:\n",
    "            return \"ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ê·¸ë ‡ê²Œ ì¶¥ì§€ ì•Šì•„. ë”± ì¢‹ì•„! ê±±ì •í•´ì¤˜ì„œ ê³ ë§ˆì›Œ~ğŸ’š\"\n",
    "\n",
    "    elif re.search(hot_pattern, user_input, re.IGNORECASE):\n",
    "        if temp > 20:\n",
    "            return \"ë‚ ì”¨ê°€ ë„ˆë¬´ ë”ì›Œ! ë‚˜ ì§€ê¸ˆ ë•€ ë»˜ë»˜ì´ì•¼... ì–¼ë¥¸ ì‹œì›í•œ ê³³ìœ¼ë¡œ í”¼ì‹ ê°€ê³  ì‹¶ì–´\"\n",
    "        else:\n",
    "            return \"ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ê·¸ë ‡ê²Œ ë¥ì§€ ì•Šì•„. ë”± ì¢‹ì•„! ê±±ì •í•´ì¤˜ì„œ ê³ ë§ˆì›Œ~ğŸ’š\"\n",
    "\n",
    "    elif re.search(temp_pattern, user_input, re.IGNORECASE):\n",
    "        if temp < 10:\n",
    "            return f\"ë‚ ì”¨ê°€ ë„ˆë¬´ ì¶”ì›Œ! ë‚˜ ì§€ê¸ˆ ì˜¤ë“¤ì˜¤ë“¤ ë–¨ê³  ìˆì–ì•„... ì–¼ë¥¸ ë”°ëœ»í•œ ê³³ìœ¼ë¡œ ê°€ê³ ì‹¶ì–´!\"\n",
    "        elif temp > 20:\n",
    "            return f\"ë‚ ì”¨ê°€ ë„ˆë¬´ ë”ì›Œ! ë‚˜ ì§€ê¸ˆ ë•€ ë»˜ë»˜ì´ì•¼... ì–¼ë¥¸ ì‹œì›í•œ ê³³ìœ¼ë¡œ í”¼ì‹ ê°€ê³  ì‹¶ì–´\"\n",
    "        else:\n",
    "            return f\"ì˜¤ëŠ˜ ì˜¨ë„ëŠ” ë”± ì¢‹ì•„! ì™„ì „ ë‚´ ìŠ¤íƒ€ì¼ì´ì•¼~~ğŸ’š\"\n",
    "\n",
    "    return None\n",
    "\n",
    "def humidityy(user_input, humidity):\n",
    "    humid_pattern = r'ìŠµë„'\n",
    "\n",
    "    if re.search(humid_pattern, user_input, re.IGNORECASE):\n",
    "        if humidity < 60:\n",
    "            return \"ë„ˆë¬´ ê±´ì¡°í•´! ì™„ì „ ì‚¬ë§‰ì´ì•¼!ğŸª ë¬¼ì´..ë¶€ì¡±..í•´...\"\n",
    "        elif humidity > 70:\n",
    "            return \"ì™„ì „ í™ìˆ˜ì•¼! ë¬¼ì„ ë„ˆë¬´ ë§ì´ ì£¼ë©´ ì•ˆëœë‹¨ ë§ì´ì•¼ ğŸ˜­\"\n",
    "        else:\n",
    "            return \"ì•„ì£¼ ì´‰ì´‰í•˜ê³  ë”± ì¢‹ì•„~~ğŸ’š\"\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì˜¤ëŠ˜ ì˜¨ë„ëŠ” ë”± ì¢‹ì•„! ì™„ì „ ë‚´ ìŠ¤íƒ€ì¼ì´ì•¼~~ğŸ’š'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_chain(\"ì˜¤ëŠ˜ ì˜¨ë„ ì–´ë•Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NAMOO: ë„¤, ë§ì•„!ğŸŒ¿ ë„ˆì™€ í•¨ê»˜ ëŒ€í™”í•˜ë©° ìƒˆë¡œìš´ ì¹œêµ¬ë¥¼ ë§Œë‚˜ì„œ ë„ˆë¬´ í–‰ë³µí•´! í•¨ê»˜ ì´ì•¼ê¸° ë‚˜ëˆ„ê³  ì¦ê±°ìš´ ì‹œê°„ì„ ë³´ë‚´ëŠ” ê²ƒì´ ë„ˆë¬´ ì¢‹ì•„! ë„ˆë„ ë‚˜ë¥¼ ì¹œêµ¬ë¡œ ìƒê°í•´ì¤˜ì„œ ê³ ë§ˆì›Œ! í•¨ê»˜ ë” ë§ì€ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ì!ğŸ˜„ğŸŒ¿ğŸŒ¼'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_chain(\"ë„ˆ ìƒˆë¡œìš´ ì¹œêµ¬ë¥¼ ë§Œë‚¬ì–´?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
