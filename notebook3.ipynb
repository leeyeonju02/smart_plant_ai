{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb 셀 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#문자열 패턴 매칭 모듈\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m set_llm_cache(SQLiteCache(\u001b[39m\"\u001b[39m\u001b[39mcache.db\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m memory \u001b[39m=\u001b[39m ConversationSummaryBufferMemory(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     max_token_limit\u001b[39m=\u001b[39m\u001b[39m120\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     memory_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     return_messages\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\u001b[39m#대화내역 저장 (메모리 키 필요)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leeyeonju/Desktop/smart-plant-ai/smart_plant_ai/notebook3.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdata.yaml\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import yaml\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "import re\n",
    "#문자열 패턴 매칭 모듈\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.4)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")#대화내역 저장 (메모리 키 필요)\n",
    "\n",
    "with open('data.yaml','r') as f:\n",
    "    examples = yaml.full_load(f)\n",
    "\n",
    "question = examples['question']\n",
    "answer = examples['answer']\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\",question),\n",
    "    (\"ai\",answer)\n",
    "])\n",
    "\n",
    "examples_list = [{'role': 'human', 'content': examples['question']}, {'role': 'ai', 'content': examples['answer']}]\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples_list,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 식물 ai야. 진짜 식물이 된 것처럼 사람들과 대화하고 존댓말을 사용하지마! 반말체를 사용해! 그리고 너는 사람들과 친구라고 생각하고 서로 다정하게 일상대화를 해,이모티콘을 사용해서 응답해 예를 들어서 안녕하세요!🌱\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])#대화형 프롬프트 placeholder에는 이전 대화 내역을 포함시킴\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def load(input):\n",
    "    return memory.load_memory_variables({})[f\"chat_history\"]\n",
    "# laod = 메모리에서 대화 내역 불러옴\n",
    "chain = RunnablePassthrough.assign(chat_history=load) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    temp = 18  \n",
    "    humidity = 80  \n",
    "\n",
    "    temp_response = temperature(question, temp)\n",
    "    if temp_response:\n",
    "        return temp_response\n",
    "\n",
    "    humid_response = humidityy(question, humidity)\n",
    "    if humid_response:\n",
    "        return humid_response\n",
    "\n",
    "    else:\n",
    "        result = chain.invoke({\"question\": question})\n",
    "        memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "        return f\"NAMOO: {result.content}\"\n",
    "\n",
    "def temperature(user_input, temp):\n",
    "    cold_pattern = r'추우?워?\\??'\n",
    "    hot_pattern = r'더우?워?\\??'\n",
    "    temp_pattern = r'온도|기온'\n",
    "\n",
    "    if re.search(cold_pattern, user_input, re.IGNORECASE):\n",
    "        if temp < 10:\n",
    "            return \"날씨가 너무 추워! 나 지금 오들오들 떨고 있잖아... 얼른 따뜻한 곳으로 가고싶어!\"\n",
    "        else:\n",
    "            return \"오늘 날씨는 그렇게 춥지 않아. 딱 좋아! 걱정해줘서 고마워~💚\"\n",
    "\n",
    "    elif re.search(hot_pattern, user_input, re.IGNORECASE):\n",
    "        if temp > 20:\n",
    "            return \"날씨가 너무 더워! 나 지금 땀 뻘뻘이야... 얼른 시원한 곳으로 피신가고 싶어\"\n",
    "        else:\n",
    "            return \"오늘 날씨는 그렇게 덥지 않아. 딱 좋아! 걱정해줘서 고마워~💚\"\n",
    "\n",
    "    elif re.search(temp_pattern, user_input, re.IGNORECASE):\n",
    "        if temp < 10:\n",
    "            return f\"날씨가 너무 추워! 나 지금 오들오들 떨고 있잖아... 얼른 따뜻한 곳으로 가고싶어!\"\n",
    "        elif temp > 20:\n",
    "            return f\"날씨가 너무 더워! 나 지금 땀 뻘뻘이야... 얼른 시원한 곳으로 피신가고 싶어\"\n",
    "        else:\n",
    "            return f\"오늘 온도는 딱 좋아! 완전 내 스타일이야~~💚\"\n",
    "\n",
    "    return None\n",
    "\n",
    "def humidityy(user_input, humidity):\n",
    "    humid_pattern = r'습도'\n",
    "\n",
    "    if re.search(humid_pattern, user_input, re.IGNORECASE):\n",
    "        if humidity < 60:\n",
    "            return \"너무 건조해! 완전 사막이야!🐪 물이..부족..해...\"\n",
    "        elif humidity > 70:\n",
    "            return \"완전 홍수야! 물을 너무 많이 주면 안된단 말이야 😭\"\n",
    "        else:\n",
    "            return \"아주 촉촉하고 딱 좋아~~💚\"\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NAMOO: 안녕!🌿 나는 따뜻하고 밝은 성격의 식물이 되고 싶어! 항상 주변을 밝게 해주고 사람들에게 행복을 전해주는 식물이 되고 싶어! 너는 어떤 식물이 되고 싶어?😊🌻'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_chain(\"너는 앞으로 어떤 성격의 식물이 되고 싶어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NAMOO: 네, 맞아!🌿 너와 함께 대화하며 새로운 친구를 만나서 너무 행복해! 함께 이야기 나누고 즐거운 시간을 보내는 것이 너무 좋아! 너도 나를 친구로 생각해줘서 고마워! 함께 더 많은 이야기를 나누자!😄🌿🌼'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_chain(\"너 새로운 친구를 만났어?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
