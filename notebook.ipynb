{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지데이터와 라벨을 분류하기 위한 CustomDataset 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='Training', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "        # 이미지 및 라벨 리스트 초기화\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # 데이터 폴더 경로 설정\n",
    "        data_folder = 'Training' if mode == 'Training' else 'Validation'\n",
    "        data_path = os.path.join(root_dir, data_folder)\n",
    "\n",
    "        # 이미지와 라벨 로드\n",
    "        self._load_data(data_path)\n",
    "\n",
    "    #주어진 데이터 폴더에서 이미지와 라벨 로드\n",
    "    def _load_data(self, data_path): \n",
    "        label_folder_path = os.path.join(data_path, \"라벨링데이터\", \"05.상추\")\n",
    "        image_folder_path = os.path.join(data_path, \"원천데이터\", \"05.상추\")\n",
    "\n",
    "        label_folders = os.listdir(label_folder_path)\n",
    "\n",
    "        for label_folder in label_folders:\n",
    "            label = int(label_folder.split(\".\")[0].split()[-1])  # 라벨 추출\n",
    "            label_path = os.path.join(label_folder_path, label_folder)\n",
    "            image_path = os.path.join(image_folder_path, label_folder)\n",
    "\n",
    "            # 이미지 파일과 라벨 파일 매칭\n",
    "            for image_filename in os.listdir(image_path):\n",
    "                image_file_path = os.path.join(image_path, image_filename)\n",
    "                self.images.append(image_file_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    #데이터 셋의 총 데이터수\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    #주어진 인덱스에 해당하는 라벨과 이미지 출력 \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 이미지 불러오기\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # 이미지 전처리 적용\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# 데이터셋의 폴더 경로 설정\n",
    "root_dir = \"/Volumes/T7/상추 질병 진단/data\"\n",
    "\n",
    "# 데이터 전처리를 위한 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), #resnet이 훈련될때 224 * 224를 사용하기 때문에 조정\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(root_dir, mode='Training', transform=transform)\n",
    "val_dataset = CustomDataset(root_dir, mode='Validation', transform=transform)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='Training', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        data_folder = 'Training' if mode == 'Training' else 'Validation'\n",
    "        data_path = os.path.join(root_dir, data_folder)\n",
    "\n",
    "        self._load_data(data_path)\n",
    "\n",
    "    def _load_data(self, data_path):\n",
    "        image_folder_path = os.path.join(data_path, \"원천데이터\", \"05.상추\")\n",
    "        label_folder_path = os.path.join(data_path, \"라벨링데이터\", \"05.상추\")\n",
    "\n",
    "        image_folders = os.listdir(image_folder_path)\n",
    "\n",
    "        for image_folder in image_folders:\n",
    "            image_path = os.path.join(image_folder_path, image_folder)\n",
    "            label_path = os.path.join(label_folder_path, image_folder)\n",
    "\n",
    "            for image_filename in os.listdir(image_path):\n",
    "                image_file_path = os.path.join(image_path, image_filename)\n",
    "                label_file_path = os.path.join(label_path, image_filename.replace('.jpeg', '.json'))\n",
    "\n",
    "                # JSON 파일에서 disease 값을 읽어서 라벨 결정\n",
    "                if os.path.exists(label_file_path):\n",
    "                    with open(label_file_path, 'r') as f:\n",
    "                        label_info = json.load(f)\n",
    "                        disease = label_info['annotations']['disease']\n",
    "\n",
    "                        if disease == 9:  # 균핵병\n",
    "                            label = 1\n",
    "                        elif disease == 10:  # 노균병\n",
    "                            label = 2\n",
    "                        else:  # 정상\n",
    "                            label = 0\n",
    "                else:\n",
    "                    label = 0  # 라벨 정보가 없는 경우 정상으로 처리\n",
    "\n",
    "                self.images.append(image_file_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn #신경망\n",
    "\n",
    "class BasicBlock(nn.Module): #모듈정의\n",
    "  def __init__(self, in_channels, out_channels, kernel_size=3): #입력채널수 , 출력채널수 , 합성곱이 갖는 커널의 크기\n",
    "    super(BasicBlock, self).__init__() #nn.Module의 요소를 가져와라\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "\n",
    "    # 합성곱층 정의\n",
    "    self.c1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1)\n",
    "    self.c2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1) # c1의 출력을 입력으로 받는다.\n",
    "\n",
    "    # 다운샘플링이 필요한 경우에만 다운샘플 레이어 정의\n",
    "    self.downsample = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None\n",
    "\n",
    "    # 합성곱 하나 거칠때마다 배치정규화도 필요\n",
    "    self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "    self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "    # 활성화 함수 정의\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  # 순전파 정의\n",
    "  def forward(self, x):\n",
    "    identity = x\n",
    "\n",
    "    # Resnet의 기본 블록에서 F(x) 부분\n",
    "    x = self.c1(x) # 합성곱\n",
    "    x = self.bn1(x) # 배치 정규화\n",
    "    x = self.relu(x) # 활성화\n",
    "    x = self.c2(x)\n",
    "    x = self.bn2(x)\n",
    "\n",
    "    # 입력과 출력 채널이 다를 경우에만 다운샘플 적용\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "\n",
    "    x += identity # 은닉층을 거친 x + 다운샘플을 거친 identity\n",
    "    x = self.relu(x) \n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):  # 3개의 클래스로 수정\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        # 기본 블록 3\n",
    "        self.b1 = BasicBlock(in_channels=3, out_channels=64)\n",
    "        self.b2 = BasicBlock(in_channels=64, out_channels=128)\n",
    "        self.b3 = BasicBlock(in_channels=128, out_channels=256)\n",
    "       \n",
    "\n",
    "        # 평균값 풀링\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 분류기 층\n",
    "        self.fc1 = nn.Linear(in_features=4096, out_features=2048) #4*4*256 -> 원래 32*32크기 이미지가 4*4까지 떨어지게 됐기 때문\n",
    "        self.fc2 = nn.Linear(in_features=2048, out_features=512)\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "\n",
    "\n",
    "        self.relu = nn.ReLU() #활성화 함수\n",
    "\n",
    "      #순전파\n",
    "    def forward(self, x):\n",
    "    #기본블록과 풀링층 통과\n",
    "       x = self.b1(x) #첫번째 블록\n",
    "       x = self.pool(x) #풀링\n",
    "       x = self.b2(x)\n",
    "       x = self.pool(x)\n",
    "       x = self.b3(x)\n",
    "       x = self.pool(x) #여기까지는 2차원 이미지\n",
    "\n",
    "       print(\"크기 출력\" + x.shape) \n",
    "    #분류기의 입력으로 사용하기 위한 평탄화\n",
    "       x = torch.flatten(x, start_dim=1)\n",
    " \n",
    "    #분류기로 예측값 출력\n",
    "       x = self.fc1(x) #분류기\n",
    "       x = self.relu(x) #활성화\n",
    "       x = self.fc2(x)\n",
    "       x = self.relu(x)\n",
    "       x = self.fc3(x) #결국 10개를 갖게 된다.\n",
    "\n",
    "       return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (b1): BasicBlock(\n",
       "    (c1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (c2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (downsample): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (b2): BasicBlock(\n",
       "    (c1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (c2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (b3): BasicBlock(\n",
       "    (c1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (c2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (fc1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터셋과 데이터로더\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = ResNet(num_classes=3)\n",
    "\n",
    "# 손실 함수 및 최적화기\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# 장치 설정 (GPU 사용 가능 시)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x200704 and 4096x2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/T7/상추 질병 진단/notebook.ipynb 셀 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)  \u001b[39m# 데이터를 GPU로 이동\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# 그래디언트 초기화\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)  \u001b[39m# 모델 순전파\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)  \u001b[39m# 손실 계산 - 크로스 엔트로피 \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# 역전파\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Volumes/T7/상추 질병 진단/notebook.ipynb 셀 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m    x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m#분류기로 예측값 출력\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m    x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x) \u001b[39m#분류기\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m    x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x) \u001b[39m#활성화\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m    x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x200704 and 4096x2048)"
     ]
    }
   ],
   "source": [
    "# 학습 루프\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 모델을 훈련 모드로 설정\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)  # 데이터를 GPU로 이동\n",
    "        optimizer.zero_grad()  # 그래디언트 초기화\n",
    "        outputs = model(images)  # 모델 순전파\n",
    "        loss = criterion(outputs, labels)  # 손실 계산 - 크로스 엔트로피 \n",
    "        loss.backward()  # 역전파\n",
    "        optimizer.step()  # 매개변수 업데이트\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    \n",
    "    # 검증 데이터셋을 사용하여 모델 평가\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    val_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # 데이터를 GPU로 이동\n",
    "            outputs = model(images)  # 모델 순전파\n",
    "            loss = criterion(outputs, labels)  # 손실 계산\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "    val_acc = correct_preds / total_preds\n",
    "    \n",
    "    # 에포크마다 결과 출력\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 레이블: {'Validation': 0, 'Training': 1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_class_labels(dataset_root):\n",
    "    # 클래스 레이블 딕셔너리 초기화\n",
    "    class_labels = {}\n",
    "\n",
    "    # 데이터셋 루트 디렉토리 내의 하위 디렉토리(클래스별로 구분된 폴더) 탐색\n",
    "    for class_name in os.listdir(dataset_root):\n",
    "        class_path = os.path.join(dataset_root, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            # 클래스 레이블 딕셔너리에 클래스명과 해당 클래스의 인덱스 추가\n",
    "            class_labels[class_name] = len(class_labels)\n",
    "\n",
    "    return class_labels\n",
    "\n",
    "# 데이터셋 루트 디렉토리 경로 설정\n",
    "dataset_root = '/Volumes/T7/상추 질병 진단/data'\n",
    "\n",
    "# 클래스 레이블 확인\n",
    "class_labels = get_class_labels(dataset_root)\n",
    "print(\"클래스 레이블:\", class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 레이블: {'05.상추_0.정상': 0, '05.상추_1.질병': 1, '05.상추_9.증강': 2}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_class_labels(dataset_root):\n",
    "    # 클래스 레이블 딕셔너리 초기화\n",
    "    class_labels = {}\n",
    "\n",
    "    # 데이터셋 루트 디렉토리 내의 하위 디렉토리(클래스별로 구분된 폴더) 탐색\n",
    "    for class_name in os.listdir(dataset_root):\n",
    "        class_path = os.path.join(dataset_root, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            # 클래스 레이블 딕셔너리에 클래스명과 해당 클래스의 인덱스 추가\n",
    "            class_labels[class_name] = len(class_labels)\n",
    "\n",
    "    return class_labels\n",
    "\n",
    "# 데이터셋 루트 디렉토리 경로 설정\n",
    "dataset_root = '/Volumes/T7/상추 질병 진단/data/Training/라벨링데이터/05.상추'\n",
    "\n",
    "# 클래스 레이블 확인\n",
    "class_labels = get_class_labels(dataset_root)\n",
    "print(\"클래스 레이블:\", class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 로더중 none 값을 걸래는 dataset\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='Training', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        data_folder = 'Training' if mode == 'Training' else 'Validation'\n",
    "        data_path = os.path.join(root_dir, data_folder)\n",
    "\n",
    "        self._load_data(data_path)\n",
    "\n",
    "    def _load_data(self, data_path):\n",
    "        image_folder_path = os.path.join(data_path, \"원천데이터\", \"05.상추\")\n",
    "        label_folder_path = os.path.join(data_path, \"라벨링데이터\", \"05.상추\")\n",
    "\n",
    "        image_folders = os.listdir(image_folder_path)\n",
    "\n",
    "        for image_folder in image_folders:\n",
    "            image_path = os.path.join(image_folder_path, image_folder)\n",
    "            label_path = os.path.join(label_folder_path, image_folder)\n",
    "\n",
    "            for image_filename in os.listdir(image_path):\n",
    "                image_file_path = os.path.join(image_path, image_filename)\n",
    "                label_file_path = os.path.join(label_path, image_filename.replace('.jpeg', '.json'))\n",
    "\n",
    "                # JSON 파일에서 disease 값을 읽어서 라벨 결정\n",
    "                if os.path.exists(label_file_path):\n",
    "                    with open(label_file_path, 'r') as f:\n",
    "                        label_info = json.load(f)\n",
    "                        disease = label_info['annotations']['disease']\n",
    "\n",
    "                        if disease == 9:  # 균핵병\n",
    "                            label = 1\n",
    "                        elif disease == 10:  # 노균병\n",
    "                            label = 2\n",
    "                        else:  # 정상\n",
    "                            label = 0\n",
    "                else:\n",
    "                    label = 0  # 라벨 정보가 없는 경우 정상으로 처리\n",
    "\n",
    "                self.images.append(image_file_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except (IOError, UnidentifiedImageError) as e:\n",
    "            print(f\"Warning: Could not read image {image_path}. Skipping. Error: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# DataLoader에서 None 타입 데이터를 걸러내기 위한 collate_fn\n",
    "def my_collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
    "    return torch.utils.data.DataLoader.default_collate(batch)\n",
    "\n",
    "# CustomDataset과 DataLoader 인스턴스 생성 예시\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(root_dir=\"/Volumes/T7/상추 질병 진단/data\", mode='Training', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=my_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/T7/상추 질병 진단/notebook.ipynb 셀 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X16sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/Volumes/T7/상추 질병 진단/notebook.ipynb 셀 13\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X16sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages[idx]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X16sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X16sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(image_path)\u001b[39m.\u001b[39;49mconvert(\u001b[39m'\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X16sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X16sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:933\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[1;32m    886\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[1;32m    887\u001b[0m ):\n\u001b[1;32m    888\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    935\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    937\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models.resnet import ResNet50_Weights  # 추가된 부분\n",
    "\n",
    "# 데이터 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet50에 맞는 이미지 크기로 조정\n",
    "    transforms.ToTensor(),          # 이미지를 Tensor로 변환\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet 평균 및 표준편차로 정규화\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# CustomDataset 인스턴스 생성\n",
    "root_dir = '/Volumes/T7/상추 질병 진단/data'  # 데이터셋 경로 지정\n",
    "train_dataset = CustomDataset(root_dir=root_dir, mode='Training', transform=transform)\n",
    "val_dataset = CustomDataset(root_dir=root_dir, mode='Validation', transform=transform)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# ResNet50 모델 로드 및 수정\n",
    "weights = ResNet50_Weights.DEFAULT  # 사전 훈련된 가중치를 지정하는 부분\n",
    "model = models.resnet50(weights=weights)  # 수정된 부분\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3)  # 마지막 레이어를 3개 클래스 출력으로 교체\n",
    "\n",
    "# 손실 함수 및 최적화 알고리즘 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 훈련 루프\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    print(f'Accuracy on validation set: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeyeonju/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/leeyeonju/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/T7/상추 질병 진단/notebook.ipynb 셀 14\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X20sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X20sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X20sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/T7/%EC%83%81%EC%B6%94%20%EC%A7%88%EB%B3%91%20%EC%A7%84%EB%8B%A8/notebook.ipynb#X20sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/models/resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    144\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m--> 146\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    147\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# 데이터 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet50에 맞는 이미지 크기로 조정\n",
    "    transforms.ToTensor(),          # 이미지를 Tensor로 변환\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet 평균 및 표준편차로 정규화\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# CustomDataset 인스턴스 생성\n",
    "root_dir = '/Volumes/T7/상추 질병 진단/data'  # 데이터셋 경로 지정\n",
    "train_dataset = CustomDataset(root_dir=root_dir, mode='Training', transform=transform)\n",
    "val_dataset = CustomDataset(root_dir=root_dir, mode='Validation', transform=transform)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# ResNet50 모델 로드 및 수정\n",
    "model = models.resnet50(pretrained=True)  # 이미지넷 데이터셋에서 사전 훈련된 가중치를 사용하여 초기화\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3)  # 마지막 레이어를 3개 클래스 출력으로 교체\n",
    "\n",
    "# 손실 함수 및 최적화 알고리즘 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 훈련 루프\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on validation set: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 로더중 none 값을 걸래는 dataset\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='Training', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        data_folder = 'Training' if mode == 'Training' else 'Validation'\n",
    "        data_path = os.path.join(root_dir, data_folder)\n",
    "\n",
    "        self._load_data(data_path)\n",
    "\n",
    "    def _load_data(self, data_path):\n",
    "        image_folder_path = os.path.join(data_path, \"원천데이터\", \"05.상추\")\n",
    "        label_folder_path = os.path.join(data_path, \"라벨링데이터\", \"05.상추\")\n",
    "\n",
    "        image_folders = os.listdir(image_folder_path)\n",
    "\n",
    "        for image_folder in image_folders:\n",
    "            image_path = os.path.join(image_folder_path, image_folder)\n",
    "            label_path = os.path.join(label_folder_path, image_folder)\n",
    "\n",
    "            for image_filename in os.listdir(image_path):\n",
    "                image_file_path = os.path.join(image_path, image_filename)\n",
    "                label_file_path = os.path.join(label_path, image_filename.replace('.jpeg', '.json'))\n",
    "\n",
    "                # JSON 파일에서 disease 값을 읽어서 라벨 결정\n",
    "                if os.path.exists(label_file_path):\n",
    "                    with open(label_file_path, 'r') as f:\n",
    "                        label_info = json.load(f)\n",
    "                        disease = label_info['annotations']['disease']\n",
    "\n",
    "                        if disease == 9:  # 균핵병\n",
    "                            label = 1\n",
    "                        elif disease == 10:  # 노균병\n",
    "                            label = 2\n",
    "                        else:  # 정상\n",
    "                            label = 0\n",
    "                else:\n",
    "                    label = 0  # 라벨 정보가 없는 경우 정상으로 처리\n",
    "\n",
    "                self.images.append(image_file_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except (IOError, UnidentifiedImageError) as e:\n",
    "            print(f\"Warning: Could not read image {image_path}. Skipping. Error: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# DataLoader에서 None 타입 데이터를 걸러내기 위한 collate_fn\n",
    "def my_collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
    "    return torch.utils.data.DataLoader.default_collate(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# 데이터 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet50에 맞는 이미지 크기로 조정\n",
    "    transforms.ToTensor(),          # 이미지를 Tensor로 변환\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet 평균 및 표준편차로 정규화\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# CustomDataset 인스턴스 생성\n",
    "root_dir = '/Volumes/T7/상추 질병 진단/smalldata'  # 데이터셋 경로 지정\n",
    "train_dataset = CustomDataset(root_dir=root_dir, mode='Training', transform=transform)\n",
    "val_dataset = CustomDataset(root_dir=root_dir, mode='Validation', transform=transform)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# ResNet50 모델 로드 및 수정\n",
    "model = models.resnet50(pretrained=True)  # 이미지넷 데이터셋에서 사전 훈련된 가중치를 사용하여 초기화\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3)  # 마지막 레이어를 3개 클래스 출력으로 교체\n",
    "\n",
    "# 손실 함수 및 최적화 알고리즘 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 훈련 루프\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on validation set: {accuracy}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
